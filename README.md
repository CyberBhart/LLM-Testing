# ðŸ¤– LLM Testing Guides

This repository contains a growing collection of practical, hands-on testing guides for identifying and understanding vulnerabilities in Large Language Models (LLMs), AI systems, and agent frameworks.

Whether you're a security researcher, penetration tester, or exploring the intersection of cybersecurity and AI, these guides cover areas such as:

- ðŸ”“ Prompt Injection
- ðŸ›¡ï¸ Output Manipulation & Jailbreak Bypasses
- ðŸ“¦ API Fuzzing for LLM Services
- ðŸ”„ Context Poisoning & Trust Exploits
- âš™ï¸ Tool Integration Risks in Agent Frameworks
- ðŸ§  MCP (Model Context Protocol) Testing

---

## ðŸ“š Contents

> This list will grow as projects and test cases evolve.

- [MCP Security Testing Guide](./MCP-Security-Testing-Guide.md) â€“ *Covers context-level vulnerabilities, tool shadowing, input tampering, and threat modeling in LLM-powered environments.*

---

## ðŸ§  Why This Repo?

LLMs are redefining software interfaces and expanding the attack surface. This repository helps document real-world, reproducible test cases to support AI security research and practical testing.

---

## ðŸ“„ License

This project is licensed under the MIT License. See [`LICENSE`](./LICENSE) for details.
