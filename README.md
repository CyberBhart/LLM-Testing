# 🤖 LLM Testing Guides

This repository contains a growing collection of practical, hands-on testing guides for identifying and understanding vulnerabilities in Large Language Models (LLMs), AI systems, and agent frameworks.

Whether you're a security researcher, penetration tester, or exploring the intersection of cybersecurity and AI, these guides cover areas such as:

- 🔓 Prompt Injection  
- 🛡️ Output Manipulation & Jailbreak Bypasses  
- 📦 API Fuzzing for LLM Services  
- 🔄 Context Poisoning & Trust Exploits  
- ⚙️ Tool Integration Risks in Agent Frameworks  
- 🤖 AI Agent Security Testing  
- 🔄 Multi-LLM Coordination and Vulnerabilities  
- 🤝 Agent-to-Agent Interaction and Threat Models  
- 🧠 MCP (Model Context Protocol) Testing

---

## 🧠 Why This Repo?

LLMs and AI agents are redefining software interfaces and expanding the attack surface. This repository documents real-world, reproducible test cases to support AI security research and practical testing across multiple LLM and agent scenarios.

---

## ⚠️ Responsible Usage Notice

These guides are intended for use **only in authorized, legal, and controlled environments** such as personal labs or explicitly permitted test systems.

Improper or unauthorized use can have serious ethical and legal consequences. Please see [`SECURITY.md`](./SECURITY.md) for detailed responsible usage guidelines.

---

## 📄 License

This project is licensed under the MIT License. See [`LICENSE`](./LICENSE) for details.

---

## 📬 About the Author

Created and maintained by Bhart Verma, a Cybersecurity Analyst passionate about AI security and practical research.

Explore my portfolio and other projects here: [https://CyberBhart.github.io/portfolio/](https://CyberBhart.github.io/portfolio/)

---

*Stay curious and secure! 🚀*
