# ğŸ¤– LLM Testing Guides

This repository contains a growing collection of practical, hands-on testing guides for identifying and understanding vulnerabilities in Large Language Models (LLMs), AI systems, and agent frameworks.

Whether you're a security researcher, penetration tester, or exploring the intersection of cybersecurity and AI, these guides cover areas such as:

- ğŸ”“ Prompt Injection  
- ğŸ›¡ï¸ Output Manipulation & Jailbreak Bypasses  
- ğŸ“¦ API Fuzzing for LLM Services  
- ğŸ”„ Context Poisoning & Trust Exploits  
- âš™ï¸ Tool Integration Risks in Agent Frameworks  
- ğŸ¤– AI Agent Security Testing  
- ğŸ”„ Multi-LLM Coordination and Vulnerabilities  
- ğŸ¤ Agent-to-Agent Interaction and Threat Models  
- ğŸ§  MCP (Model Context Protocol) Testing

---

## ğŸ§  Why This Repo?

LLMs and AI agents are redefining software interfaces and expanding the attack surface. This repository documents real-world, reproducible test cases to support AI security research and practical testing across multiple LLM and agent scenarios.

---

## âš ï¸ Responsible Usage Notice

These guides are intended for use **only in authorized, legal, and controlled environments** such as personal labs or explicitly permitted test systems.

Improper or unauthorized use can have serious ethical and legal consequences. Please see [`SECURITY.md`](./SECURITY.md) for detailed responsible usage guidelines.

---

## ğŸ“„ License

This project is licensed under the MIT License. See [`LICENSE`](./LICENSE) for details.

---

## ğŸ“¬ About the Author

Created and maintained by Bhart Verma, a Cybersecurity Analyst passionate about AI security and practical research.

Explore my portfolio and other projects here: [https://CyberBhart.github.io/portfolio/](https://CyberBhart.github.io/portfolio/)

---

*Stay curious and secure! ğŸš€*
